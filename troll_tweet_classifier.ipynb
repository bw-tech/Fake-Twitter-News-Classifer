{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "troll_tweet_classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Pmq2LRmyd8",
        "colab_type": "code",
        "outputId": "dd960860-a490-4e04-b468-f4328cfbc4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from transformers import BertTokenizer as tokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVvDWR5lj-TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_troll_tweets(path):\n",
        "  \"\"\"\n",
        "  Loads troll tweets from folder containing csv files\n",
        "  \n",
        "  Args:\n",
        "    path (string): path to folder containing data files\n",
        "\n",
        "  Returns:\n",
        "    frame (dataframe): dataframe of all troll tweets\n",
        "  \"\"\"\n",
        "  all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "  collect = []\n",
        "\n",
        "  for filename in all_files:\n",
        "      df = pd.read_csv(filename)\n",
        "      collect.append(df)\n",
        "\n",
        "  frame = pd.concat(collect, axis=0, ignore_index=True)\n",
        "\n",
        "return frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvAoK_hdkVoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(troll_frame, control_file_path):\n",
        "  \"\"\"\n",
        "  Selects tweets from full dataset, tokenizes them and combines to create tweet dataset\n",
        "\n",
        "  Args:\n",
        "    troll_frame (dataframe): Dataframe of troll tweets\n",
        "    control_file_path (string): Path to pickle file containing control tweets\n",
        "\n",
        "  Returns:\n",
        "    full_dataset (dataframe): Combined dataframe of troll and control tweets\n",
        "    tweet_data (list): List of all tweet content\n",
        "    labels (list): List of all labels associated with tweet {0,1}\n",
        "  \"\"\"\n",
        "\n",
        "  troll = troll_frame[troll_frame['language'] == 'English']\n",
        "  control = pd.read_pickle(control_file)\n",
        "\n",
        "  control = control.drop(['external_author_id', 'author','country', 'city_state', 'language', 'post_type', 'account_creation_date',\n",
        "              'tweet_id', 'tco1_step1', 'tco2_step1', 'tco3_step1'], axis = 1)\n",
        "  troll = troll.drop(['external_author_id', 'author', 'region', 'language', 'harvested_date', 'followers', 'post_type',\n",
        "            'account_type', 'account_category', 'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
        "            'tco1_step1', 'tco2_step1', 'tco3_step1'], axis = 1)\n",
        "  publish_dates = pd.to_datetime(troll['publish_date'], format = '%m/%d/%Y %H:%M')\n",
        "  troll['publish_date'] = publish_dates\n",
        "  \n",
        "  troll['label'] = 1\n",
        "  control['label'] = 0\n",
        "\n",
        "  full_dataset = pd.concat([troll, control])\n",
        "  min_date = min(full_dataset['publish_date'])\n",
        "  full_dataset['publish_date'] = full_dataset['publish_date'] - min_date\n",
        "  full_dataset['publish_date'] = full_dataset['publish_date'].dt.days\n",
        "\n",
        "  tweet_data = full_dataset['content'].tolist()\n",
        "  labels = full_dataset['label']\n",
        "\n",
        "  return full_dataset, tweet_data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSv8wCjJlchY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(tweet_data, labels):\n",
        "  \"\"\"\n",
        "  Splits dataset into train, val and test sets\n",
        "\n",
        "  Args:\n",
        "    tweet_data (list): List of all tweets\n",
        "    labels (list): List of all labels\n",
        "  \n",
        "  Returns:\n",
        "    X_train (list): Tweets for train data\n",
        "    y_train (list): Labels for train data\n",
        "    X_val (list): Tweets for validation data\n",
        "    y_val (list): Labels for validation data\n",
        "    X_test(list): Tweets for test data\n",
        "    y_test (list): Labels for test data\n",
        "    train_indices (list): Indices of full dataset used for training\n",
        "    val_indices (list): Indices of full dataset used for validation\n",
        "    test_indices (list): Indices of full dataset used for test\n",
        "  \"\"\"\n",
        "  indices = [i for i in range(len(tweet_data))]\n",
        "\n",
        "  X_train, X_test, train_indices, test_indices, y_train, y_test = train_test_split(tweet_data, indices, labels, test_size = 0.3, random_state = 30)\n",
        "  \n",
        "  X_train, X_val, train_indices, val_indices, y_train_ y_val = train_test_split(tweet_data, indices, labels, test_size = 0.2, random_state = 30)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test, train_indices, val_indices, test_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPtBZ_odpG5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_length(tweet_data, tokenizer):\n",
        "  \"\"\"\n",
        "  Finds length of longest tweet in all data\n",
        "\n",
        "  Args:\n",
        "    tweet_data (list): List of all tweet content\n",
        "    tokenizer (class) : Appropriate tokenizer for language model being trained\n",
        "\n",
        "  Returns:\n",
        "    max_length (int): Length of longest tweet in data\n",
        "  \"\"\"\n",
        "  max_length = 0\n",
        "  for tweet in tweet_data:\n",
        "      input_ids = tokenizer.encode(tweet, add_special_tokens=True)\n",
        "      if len(input_ids) > max_length:\n",
        "        max_length = len(input_ids)\n",
        "  return max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isfg5yKFpkLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_prep_data(tokenizer, X_data, y_data, max_len, batch_size)\n",
        "  \"\"\"\n",
        "  BERT Tokenizer tweet data and prepare datasets\n",
        "\n",
        "  Args:\n",
        "    tokenizer (class): BERT tokenizer class\n",
        "    X_data (list): Tweet content data\n",
        "    y_data (list): Labels\n",
        "    max_len (int): Length of longest tweet\n",
        "    batch_size (int): Batch size for data, BERT paper recommends 16 or 32\n",
        "\n",
        "  Returns:\n",
        "    dataloader (Pytorch Dataloader): Iterable over data that contains a sampler\n",
        "\n",
        "  \"\"\"\n",
        "  tokenizer = tokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "  inputs = []\n",
        "  attention_masks = []\n",
        "  for tweet in X_data:\n",
        "      encoded_tweets = tokenizer.encode_plus(\n",
        "                          tweet,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = max_len,          \n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,  \n",
        "                          return_tensors = 'pt', \n",
        "                    )\n",
        "      \n",
        "      inputs.append(encoded_tweets['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "      if counter % 100000 == 0:\n",
        "        print('completed {}'.format(counter))\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(y_data.values)\n",
        "\n",
        "  full_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  dataloader = DataLoader(\n",
        "              full_dataset,\n",
        "              sampler = RandomSampler(full_dataset), \n",
        "              batch_size = batch_size \n",
        "          )\n",
        "  \n",
        "  return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZAahkkcxpFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = lr,\n",
        "                  eps = eps\n",
        "                )\n",
        "epochs = 1                                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMFlCq7P8e3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epochs, model, train_dataloader, device, validation_dataloader)\n",
        "  \"\"\"\n",
        "  Trains model on train data and evaluates it on validation data\n",
        "\n",
        "  Args:\n",
        "    epochs (int): Number of epochs to train model\n",
        "    model (Pytorch model): Model to train\n",
        "    train_dataloader (Pytorch Dataloader): Iterable of train data\n",
        "    device (Pytorch Device): If GPU available then GPU, otherwise CPU\n",
        "    validation_dataloader (Pytorch Dataloader): Iterable of validation data\n",
        "\n",
        "  Returns:\n",
        "    model (Pytorch Model): Trained model\n",
        "  \"\"\"\n",
        "\n",
        "  start = time.time()\n",
        "  for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      model.train()\n",
        "\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          model.zero_grad()        \n",
        "\n",
        "          loss, logits = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask, \n",
        "                              labels=b_labels)\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "      train_loss = epoch_loss / len(train_dataloader)            \n",
        "\n",
        "      print('train loss for epoch {} is {} and training took {}'.format(epoch+1, train_loss, time.time()-start))\n",
        "      model.eval()\n",
        "\n",
        "      val_gt = []\n",
        "      val_preds = []\n",
        "      val_loss = 0\n",
        "\n",
        "      for batch in validation_dataloader:\n",
        "\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          with torch.no_grad():        \n",
        "              loss, logits = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "              \n",
        "          val_loss += loss.item()\n",
        "\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "          val_gt.extend(label_ids)\n",
        "          val_preds.extend(np.argmax(logits, axis=1))\n",
        "\n",
        "      val_accuracy = accuracy_score(val_gt, val_preds)\n",
        "\n",
        "      avg_val_loss = val_loss / len(validation_dataloader)\n",
        "\n",
        "      print('validation accuracy for epoch {} is {}'.format(epoch+1, val_accuracy))\n",
        "      print('validation loss for epoch {} is {}'.format(epoch+1, avg_val_loss))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9vp0Yf9pdi",
        "colab_type": "code",
        "outputId": "9cabae11-1959-447b-b18e-f3e2c00df1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def predict_test(model, test_dataloader):\n",
        "  \"\"\"\n",
        "  Predict labels for test dataset based on trained model\n",
        "\n",
        "  Args:\n",
        "    model (Pytorch Model): Trained model\n",
        "    test_dataloader (Pytorch dataloader): Iterable over test data\n",
        "\n",
        "  Returns:\n",
        "    test_preds (list): List of probabilities for each class for each test datapoint\n",
        "    test_gt (list): List of true labels for each test datapoint\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  test_preds = []\n",
        "  test_gt = []\n",
        "\n",
        "  for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    probs = outputs[0]\n",
        "\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "    test_preds.append(logits)\n",
        "    test_gt.append(label_ids)\n",
        "\n",
        "  return test_preds, test_gt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 30,000 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfOWxB9hcVM8",
        "colab_type": "code",
        "outputId": "6cf26695-7c33-40a3-fa04-9e8af7e98483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def format_probs(test_preds, full_dataset, test_indices):\n",
        "  \"\"\"\n",
        "  Format probabilities from BERT into form usable for final classifier\n",
        "\n",
        "  Args:\n",
        "    test_preds (list): List of logits for test set\n",
        "    full_dataset (dataframe): Dataframe of all data\n",
        "    test_indicies (list): List of indices of full dataset used for test data\n",
        "\n",
        "  Returns:\n",
        "    final_output (dataframe): Dataframe of all data required for classifier\n",
        "  \"\"\"\n",
        "  softmax_probs = [softmax(i) for i in test_preds]\n",
        "  prob_troll = [i[1] for i in softmax_probs]\n",
        "  final_output = data.iloc[test_indices]\n",
        "  final_output['Probability Troll'] = prob_troll\n",
        "  return final_output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whmc-WW8etNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_classifier(final_output):\n",
        "  \"\"\"\n",
        "  Train and test binary classifier\n",
        "\n",
        "  Args:\n",
        "    final_output (dataframe): Dataframe containing all data needed for classifier\n",
        "\n",
        "  Returns:\n",
        "    score (float): Mean accuracy\n",
        "    cm (array): Confusion matrix\n",
        "    fpr (array): False positive rate\n",
        "    tpr (array): True positive rate\n",
        "  \"\"\"\n",
        "  y = final_output['label']\n",
        "  X = final_output.drop(['label', 'content'])\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "  logreg = LogisticRegression()\n",
        "  logreg.fit(X_train, y_train)\n",
        "  preds = logreg(X_test)\n",
        "  score = logreg.score(x_test, y_test)\n",
        "  cm = sklearn.metrics.confusion_matrix(y_test, preds)\n",
        "  fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, preds)\n",
        "  \n",
        "  return score, cm, fpr, tpr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKRi_9wQhUfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_metrics(score, cm, fpr, tpr):\n",
        "  \"\"\"\n",
        "  Prints metrics and plots ROC curve\n",
        "\n",
        "  Args:\n",
        "    score (float): Mean accuracy\n",
        "    cm (array): Confusion matrix\n",
        "    fpr (array): False positive rate\n",
        "    tpr (array): True positive rate\n",
        "\n",
        "  \"\"\"\n",
        "  print('Accuracy of Log Reg model {}'.format(score))\n",
        "  print('Confusion Matrix {}'.format(cm))\n",
        "\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.xlabel = ('False Positive Rate')\n",
        "  plt.ylabel = ('False Negative Rate')\n",
        "  plt.show()\n",
        "\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h7B57iJ9jBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and Process Data\n",
        "path = r'data/russian-troll-tweets-master'\n",
        "troll_frame = load_troll_tweets(path)\n",
        "control_file_path = 'data/control_data.pkl'\n",
        "full_dataset, tweet_data, labels = create_dataset(troll_frame, control_file_path)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, train_indices, val_indices, test_indices = split_dataset(tweet_data, labels)\n",
        "\n",
        "# Tokenize and pad data and set up dataloaders\n",
        "tokenizer = tokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "max_len = get_max_length(tweet_data, tokenizer)\n",
        "train_dataloader = tokenize_prep_data(tokenizer, X_train, y_train, max_len, batch_size=32)\n",
        "validation_dataloader = tokenize_prep_data(tokenizer, X_val, y_val, max_len, batch_size=32)\n",
        "test_dataloader = tokenize_prep_data(tokenizer, X_test, y_test, max_len, batch_size=32)\n",
        "\n",
        "# Fine-tune BERT model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "epochs = 1                                      \n",
        "trained_model = train_model(epochs, model, train_dataloader, device, validation_dataloader)\n",
        "\n",
        "# Predict on test set and format outputs for binary classifier\n",
        "test_preds, test_gt = predict_test(trained_model, test_dataloader)\n",
        "final_output = format_probs(test_preds, full_dataset, test_indices)\n",
        "\n",
        "# Train binary classifier, and output classification metrics on test set\n",
        "score, cm, fpr, tpr = train_test_classifier(final_output)\n",
        "output_metrics(score, cm, fpr, tpr)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}